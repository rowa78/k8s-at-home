---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rancher-monitoring
  namespace: cattle-monitoring-system
spec:
  interval: 1h
  chart:
    spec:
      chart: rancher-monitoring
      version: 100.1.2+up19.0.3
      sourceRef:
        kind: HelmRepository
        name: rancher-charts-release-2-6
        namespace: flux-system
  values:
    alertmanager:
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: nfs-client
              resources:
                requests:
                  storage: 10Gi        
        podAntiAffinity: "hard"
      config:
        global:
          resolve_timeout: 5m
        route:
          receiver: "rocketchat"
          group_by:
          - job
          routes:
          - receiver: "null"
            match:
              alertname: Watchdog
          - receiver: "rocketchat"
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 12h
        receivers:
        - name: "null"
        - name: 'rocketchat'
          webhook_configs:
            - send_resolved: true
              url: '${ROCKETCHAT_WEBHOOK_URL}'
        templates:
        - /etc/alertmanager/config/*.tmpl
#      config:        
#        receivers:
#        - name: 'null'
#        - name: 'rocketchat'
#          webhook_configs:
#            - send_resolved: false
#              url: '${ROCKETCHAT_WEBHOOK_URL}'                          
#        route:
#          # group_by: ['alertname', 'job']
#          # group_wait: 30s
#          # group_interval: 5m
#          # repeat_interval: 6h
#          receiver: 'rocketchat'
#          # receiver: 'pagerduty'
#          routes:
#            - match:
#                alertname: Watchdog
#              receiver: 'rocketchat'
#            - match:
#                alertname: InfoInhibitor
#              receiver: 'rocketchat'        
#            - receiver: 'rocketchat'
            
    prometheus-node-exporter:
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: instance
      resources: 
        # We usually recommend not to specify default resources and to leave this as a conscious
        # choice for the user. This also increases chances charts run on environments with little
        # resources, such as Minikube. If you do want to specify resources, uncomment the following
        # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
        limits:
          cpu: 1000m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 30Mi
    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          - action: replace
            sourceLabels:
              - node
            targetLabel: instance
    grafana:
      nodeSelector:
        kubernetes.io/arch: amd64
      persistence:
        enabled: "true"
        accessModes:
          - ReadWriteMany
        storageClassName: nfs-client
        size: 1Gi
        subPath: null
        type: pvc
      ingress:
        enabled: true
        # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
        # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
        # ingressClassName: nginx
        # Values can be templated
        annotations: 
          external-dns.alpha.kubernetes.io/hostname: "grafana.${SECRET_DOMAIN_2}"
          external-dns.alpha.kubernetes.io/ttl: "120"        
          external-dns.alpha.kubernetes.io/target: "lb.${SECRET_DOMAIN_2}" 
          kubernetes.io/ingress.class: "nginx"
          hajimari.io/enable: "true"
          hajimari.io/icon: "chart-areaspline"
          nginx.ingress.kubernetes.io/auth-url: "https://auth.${SECRET_DOMAIN_2}/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: https://auth.${SECRET_DOMAIN_2}/oauth2/start
          # kubernetes.io/ingress.class: nginx
          # kubernetes.io/tls-acme: "true"
        labels: {}
        path: /

        # pathType is only for k8s >= 1.1=
        pathType: Prefix

        hosts:
          - grafana.${SECRET_DOMAIN_2}
        ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
        extraPaths: []
        # - path: /*
        #   backend:
        #     serviceName: ssl-redirect
        #     servicePort: use-annotation
        ## Or for k8s > 1.19
        # - path: /*
        #   pathType: Prefix
        #   backend:
        #     service:
        #       name: ssl-redirect
        #       port:
        #         name: use-annotation


        tls: 
          - hosts:
              - grafana.${SECRET_DOMAIN_2}
      sidecar:
        datasources:
          enabled: true
          searchNamespace: ALL
          # defaultDatasourceEnabled: false
        dashboards:
          enabled: true
          searchNamespace: ALL
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          1-node-exporter:
            url: https://grafana.com/api/dashboards/11074/revisions/9/download
            datasource: Prometheus
          blocky:
            url: https://raw.githubusercontent.com/0xERR0R/blocky/v0.18/docs/blocky-grafana.json
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/2842          
          flux-cluster:
            url: https://raw.githubusercontent.com/fluxcd/flux2/main/manifests/monitoring/monitoring-config/dashboards/cluster.json
            datasource: Prometheus
          flux-control-plane:
            url: https://raw.githubusercontent.com/fluxcd/flux2/main/manifests/monitoring/monitoring-config/dashboards/control-plane.json
            datasource: Prometheus
          home-assistant:
            url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/grafana/dashboards/home_assistant.json
            datasource: home_assistant
          loki:
            gnetId: 13407
            revision: 1
            datasource: Prometheus
          minio-overview:
            url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/grafana/dashboards/minio_overview.json
            datasource: Prometheus
          nginx-dashboard:
            url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/grafana/dashboards/nginx.json
            datasource: Prometheus
          node-exporter-full:
            gnetId: 1860
            revision: 23
            datasource: Prometheus
          prometheus-exporter-summary:
            url: https://raw.githubusercontent.com/billimek/k8s-gitops/master/monitoring/grafana/dashboards/prometheus_exporter_summary.json
            datasource: Prometheus          
          # Ref: https://grafana.com/grafana/dashboards/11312          
          velero:
            url: https://grafana.com/api/dashboards/11055/revisions/2/download
            datasource: Prometheus
          jenkins:
            gnetId: 9964
            revision: 1
            datasource: Prometheus
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
          - name: loki
            type: loki
            access: proxy
            url: http://loki.logs.svc.cluster.local:3100            
    prometheus:
      prometheusSpec:
        resources: 
          limits:
            cpu: 3000m
            memory: 6000Mi
          requests:
            cpu: 750m
            memory: 4000Mi
        retention: 6h
        enableAdminAPI: true
        walCompression: true
#        storageSpec:
#          volumeClaimTemplate:
#            spec:
#              storageClassName: longhorn
#              resources:
#                requests:
#                  storage: 10Gi
        nodeSelector:
          kubernetes.io/arch: amd64
        podAntiAffinity: "hard"
#        storageSpec:
#          volumeClaimTemplate:
#            spec:
#              accessModes:
#                - ReadWriteMany
#              resources:
#                requests:
#                  storage: 15Gi
#              volumeMode: Filesystem
#              storageClassName: longhorn
        thanos:
          image: quay.io/thanos/thanos:v0.27.0
          version: v0.24.0
          objectStorageConfig:
            name: thanos-objstore-secret
            key: objstore.yml
        additionalScrapeConfigs:        
        - job_name: minio-job
          bearer_token: ${MINIO_PROMETHEUS_BEARER_TOKEN}
          metrics_path: /minio/v2/metrics/cluster
          scheme: https
          static_configs:
          - targets: 
            - minio.${SECRET_DOMAIN}
      thanosService:
        enabled: true
      thanosServiceMonitor:
        enabled: true
      thanosIngress:
        enabled: false
    k3sControllerManager:
      enabled: true
    k3sProxy:
      enabled: true
    k3sScheduler:
      enabled: true
    ingressNginx:
      enabled: true
      namespace: ingress-nginx
      service:
        port: 9913
        targetPort: 10254
      serviceMonitor:
        interval: 30s
        metricRelabelings: []
        proxyUrl: ''
        relabelings: []
    k3sServer:
      clients:
        https:
          enabled: true
          insecureSkipVerify: true
          useServiceAccountCredentials: true
        port: 10013
        rbac:
          additionalRules:
            - nonResourceURLs:
                - /metrics/cadvisor
              verbs:
                - get
            - apiGroups:
                - ''
              resources:
                - nodes/metrics
              verbs:
                - get
        tolerations:
          - effect: NoExecute
            operator: Exists
          - effect: NoSchedule
            operator: Exists
        useLocalhost: true
      component: k3s-server
      enabled: true
      metricsPort: 10250
      serviceMonitor:
        endpoints:
          - honorLabels: true
            port: metrics
            relabelings:
              - sourceLabels:
                  - __metrics_path__
                targetLabel: metrics_path
          - honorLabels: true
            path: /metrics/cadvisor
            port: metrics
            relabelings:
              - sourceLabels:
                  - __metrics_path__
                targetLabel: metrics_path
          - honorLabels: true
            path: /metrics/probes
            port: metrics
            relabelings:
              - sourceLabels:
                  - __metrics_path__
                targetLabel: metrics_path